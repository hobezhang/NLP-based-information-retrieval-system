{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hobezhang/NLP-based-information-retrieval-system/blob/main/Splade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F0h-8CUL6n67"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cVyGIlRlqeqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a92d956-a552-40c3-f80f-22d535445bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文档集: \n",
            "['sun sets horizon painting sky warm hues', 'bustling city people hurry workplaces early morning', 'melody distant guitar fills air quiet evening', 'exploring pages captivating novel lost world imagination', 'raindrops tap gently window pane creating soothing rhythm']\n",
            "特征名称:\n",
            " ['air' 'bustling' 'captivating' 'city' 'creating' 'distant' 'early'\n",
            " 'evening' 'exploring' 'fills' 'gently' 'guitar' 'horizon' 'hues' 'hurry'\n",
            " 'imagination' 'lost' 'melody' 'morning' 'novel' 'pages' 'painting' 'pane'\n",
            " 'people' 'quiet' 'raindrops' 'rhythm' 'sets' 'sky' 'soothing' 'sun' 'tap'\n",
            " 'warm' 'window' 'workplaces' 'world']\n",
            "\n",
            "  (0, 32)\t0.3779644730092272\n",
            "  (0, 30)\t0.3779644730092272\n",
            "  (0, 28)\t0.3779644730092272\n",
            "  (0, 27)\t0.3779644730092272\n",
            "  (0, 21)\t0.3779644730092272\n",
            "  (0, 13)\t0.3779644730092272\n",
            "  (0, 12)\t0.3779644730092272\n",
            "  (1, 34)\t0.3779644730092272\n",
            "  (1, 23)\t0.3779644730092272\n",
            "  (1, 18)\t0.3779644730092272\n",
            "  (1, 14)\t0.3779644730092272\n",
            "  (1, 6)\t0.3779644730092272\n",
            "  (1, 3)\t0.3779644730092272\n",
            "  (1, 1)\t0.3779644730092272\n",
            "  (2, 24)\t0.3779644730092272\n",
            "  (2, 17)\t0.3779644730092272\n",
            "  (2, 11)\t0.3779644730092272\n",
            "  (2, 9)\t0.3779644730092272\n",
            "  (2, 7)\t0.3779644730092272\n",
            "  (2, 5)\t0.3779644730092272\n",
            "  (2, 0)\t0.3779644730092272\n",
            "  (3, 35)\t0.3779644730092272\n",
            "  (3, 20)\t0.3779644730092272\n",
            "  (3, 19)\t0.3779644730092272\n",
            "  (3, 16)\t0.3779644730092272\n",
            "  (3, 15)\t0.3779644730092272\n",
            "  (3, 8)\t0.3779644730092272\n",
            "  (3, 2)\t0.3779644730092272\n",
            "  (4, 33)\t0.35355339059327373\n",
            "  (4, 31)\t0.35355339059327373\n",
            "  (4, 29)\t0.35355339059327373\n",
            "  (4, 26)\t0.35355339059327373\n",
            "  (4, 25)\t0.35355339059327373\n",
            "  (4, 22)\t0.35355339059327373\n",
            "  (4, 10)\t0.35355339059327373\n",
            "  (4, 4)\t0.35355339059327373\n",
            "TF-IDF 矩阵:\n",
            " [[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.37796447 0.37796447 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.37796447 0.         0.\n",
            "  0.         0.         0.         0.37796447 0.37796447 0.\n",
            "  0.37796447 0.         0.37796447 0.         0.         0.        ]\n",
            " [0.         0.37796447 0.         0.37796447 0.         0.\n",
            "  0.37796447 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.37796447 0.         0.         0.\n",
            "  0.37796447 0.         0.         0.         0.         0.37796447\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.37796447 0.        ]\n",
            " [0.37796447 0.         0.         0.         0.         0.37796447\n",
            "  0.         0.37796447 0.         0.37796447 0.         0.37796447\n",
            "  0.         0.         0.         0.         0.         0.37796447\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.37796447 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.37796447 0.         0.         0.\n",
            "  0.         0.         0.37796447 0.         0.         0.\n",
            "  0.         0.         0.         0.37796447 0.37796447 0.\n",
            "  0.         0.37796447 0.37796447 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.37796447]\n",
            " [0.         0.         0.         0.         0.35355339 0.\n",
            "  0.         0.         0.         0.         0.35355339 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.35355339 0.\n",
            "  0.         0.35355339 0.35355339 0.         0.         0.35355339\n",
            "  0.         0.35355339 0.         0.35355339 0.         0.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = ['The sun sets over the horizon, painting the sky in warm hues.',\n",
        "          'In a bustling city, people hurry to their workplaces in the early morning.',\n",
        "          'A melody from a distant guitar fills the air on a quiet evening.',\n",
        "          'Exploring the pages of a captivating novel, lost in a world of imagination.',\n",
        "          'Raindrops tap gently on the window pane, creating a soothing rhythm.'\n",
        "         ]\n",
        "\n",
        "# 定义用于预处理文本的函数\n",
        "def preprocess_text(text):\n",
        "    # 移除标点符号和其他非字母数字字符\n",
        "    text =  re.sub('[^a-zA-Z]', ' ', text)\n",
        "    # 将文本标记化为单词\n",
        "    words = word_tokenize(text.lower())\n",
        "    # 移除停用词\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # print(stopwords.words('english'))\n",
        "    # 将单词重新连接成字符串\n",
        "    return ' '.join(words)\n",
        "\n",
        "# 预处理样例文档\n",
        "corpus = [preprocess_text(doc) for doc in corpus]\n",
        "print('文档集: \\n{}'.format(corpus))\n",
        "\n",
        "# 创建一个 TfidfVectorizer 对象并将其拟合到预处理的文档集\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# 获取与 TF-IDF 矩阵中的列对应的特征名称列表\n",
        "print(\"特征名称:\\n\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# 将预处理的文档集转换为 TF-IDF 矩阵\n",
        "tf_idf_matrix = vectorizer.transform(corpus)\n",
        "\n",
        "# 打印生成的矩阵\n",
        "print()\n",
        "print(tf_idf_matrix)\n",
        "print(\"TF-IDF 矩阵:\\n\",tf_idf_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class BM25:\n",
        "    def __init__(self, corpus, k1=1.5, b=0.75):\n",
        "        \"\"\"\n",
        "\n",
        "        参数：\n",
        "        - corpus: 文档集合，每个文档应该是一个单词列表\n",
        "        - k1: 一个调节参数，控制词项频率对得分的影响\n",
        "        - b: 一个调节参数，控制文档长度对得分的影响\n",
        "        \"\"\"\n",
        "        self.corpus = corpus\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.doc_lengths = [len(doc) for doc in corpus]\n",
        "        self.avg_doc_length = sum(self.doc_lengths) / len(corpus)\n",
        "        self.doc_count = len(corpus)\n",
        "        self.idf = self.calculate_idf()\n",
        "\n",
        "    def calculate_idf(self):\n",
        "        \"\"\"\n",
        "        计算每个词项的逆文档频率（IDF）\n",
        "\n",
        "        返回一个字典，其中键是词项，值是对应的IDF值\n",
        "        \"\"\"\n",
        "        idf = {}\n",
        "        for doc in self.corpus:\n",
        "            unique_terms = set(doc)\n",
        "            for term in unique_terms:\n",
        "                idf[term] = idf.get(term, 0) + 1\n",
        "\n",
        "        for term in idf:\n",
        "            idf[term] = math.log((self.doc_count - idf[term] + 0.5) / (idf[term] + 0.5) + 1.0)\n",
        "\n",
        "        return idf\n",
        "\n",
        "    def calculate_score(self, query, doc):\n",
        "        \"\"\"\n",
        "        计算查询与文档之间的BM25得分\n",
        "\n",
        "        参数：\n",
        "        - query: 查询词项列表\n",
        "        - doc: 文档词项列表\n",
        "\n",
        "        返回BM25得分\n",
        "        \"\"\"\n",
        "        score = 0.0\n",
        "\n",
        "        for term in query:\n",
        "            if term in doc:\n",
        "                df = sum([1 for d in self.corpus if term in d])\n",
        "                idf_term = self.idf.get(term, 0)\n",
        "                score += (idf_term * doc.count(term) * (self.k1 + 1)) / (\n",
        "                        doc.count(term) + self.k1 * (1 - self.b + self.b * len(doc) / self.avg_doc_length))\n",
        "                score *= df / (df + 1)  # length normalization\n",
        "\n",
        "        return score\n",
        "\n",
        "    def rank_documents(self, query):\n",
        "        \"\"\"\n",
        "        对文档进行排名，返回排名后的文档列表和对应的BM25得分\n",
        "\n",
        "        参数：\n",
        "        - query: 查询词项列表\n",
        "\n",
        "        返回排名后的文档索引列表和对应的BM25得分\n",
        "        \"\"\"\n",
        "        scores = [(i, self.calculate_score(query, self.corpus[i])) for i in range(self.doc_count)]\n",
        "        ranked_docs = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "        return [(doc[0], doc[1]) for doc in ranked_docs]\n",
        "\n",
        "\n",
        "# 示例用法\n",
        "corpus = [\n",
        "    [\"apple\", \"banana\", \"orange\", \"apple\"],\n",
        "    [\"banana\", \"orange\", \"orange\",\"apple\"],\n",
        "    [\"apple\", \"apple\", \"banana\"]\n",
        "]\n",
        "\n",
        "query = [\"apple\", \"orange\"]\n",
        "\n",
        "bm25_model = BM25(corpus)\n",
        "result = bm25_model.rank_documents(query)\n",
        "print(\"排名后的文档索引和BM25得分：\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRwUEqW3-koU",
        "outputId": "85eafc23-f592-4d00-8f01-47efac71a1eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "排名后的文档索引和BM25得分： [(1, 0.499055695673375), (0, 0.39369287621150584), (2, 0.15194951574514637)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPLADE"
      ],
      "metadata": {
        "id": "6KUAbkHB9msK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "model_id = 'naver/splade-cocondenser-ensembledistil'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "7Pl085P79btp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(\"The dogs like fishes.\", return_tensors='pt')\n",
        "output = model(**tokens)\n",
        "output.logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG9ydWG39os3",
        "outputId": "56af0009-4cbf-45a9-abd9-f39dcb684ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 30522])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "vec = torch.max(\n",
        "    torch.log(\n",
        "        1 + torch.relu(output.logits)\n",
        "    ) * tokens.attention_mask.unsqueeze(-1),\n",
        "dim=1).values.squeeze()\n",
        "\n"
      ],
      "metadata": {
        "id": "uIfKcfTP-TnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euh9Fk1f-m2_",
        "outputId": "720c16b0-c581-4618-ec4a-51c415497e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0.,  ..., 0., 0., 0.], grad_fn=<SqueezeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cols = vec.nonzero().squeeze().cpu().tolist()\n",
        "print(len(cols))\n",
        "\n",
        "\n",
        "weights = vec[cols].cpu().tolist()\n",
        "\n",
        "sparse_dict = dict(zip(cols, weights))\n",
        "sparse_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in-L79PI-rJ4",
        "outputId": "3893ec0f-622e-435c-9081-d76cbfe3be1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1024: 0.3254936933517456,\n",
              " 1996: 0.13122430443763733,\n",
              " 2027: 0.5024430155754089,\n",
              " 2066: 1.8275706768035889,\n",
              " 2123: 0.0026543643325567245,\n",
              " 2215: 0.6085304021835327,\n",
              " 2298: 0.0978935956954956,\n",
              " 2300: 0.05859898775815964,\n",
              " 2427: 0.12191396951675415,\n",
              " 2748: 0.058032430708408356,\n",
              " 3869: 2.25282621383667,\n",
              " 3899: 2.5785505771636963,\n",
              " 4111: 0.5009175539016724,\n",
              " 4153: 0.28826817870140076,\n",
              " 4176: 0.2985142469406128,\n",
              " 4521: 0.7205590605735779,\n",
              " 4669: 1.048478364944458,\n",
              " 4743: 0.1843896210193634,\n",
              " 5074: 0.031307652592659,\n",
              " 5248: 0.6992028951644897,\n",
              " 5437: 0.23216332495212555,\n",
              " 5438: 0.1383255124092102,\n",
              " 5532: 0.4857339560985565,\n",
              " 5645: 1.7455496788024902,\n",
              " 5742: 0.6805999875068665,\n",
              " 5875: 0.2832365036010742,\n",
              " 5933: 0.3116060197353363,\n",
              " 6077: 2.5794029235839844,\n",
              " 6240: 0.009563819505274296,\n",
              " 6805: 0.25153931975364685,\n",
              " 6816: 0.1546255648136139,\n",
              " 7488: 0.07079166173934937,\n",
              " 7529: 0.0016832482069730759,\n",
              " 8843: 0.31764745712280273,\n",
              " 9004: 0.28628262877464294,\n",
              " 9201: 0.19111168384552002,\n",
              " 9544: 0.5172513127326965,\n",
              " 9880: 0.08606124669313431,\n",
              " 9958: 0.18911141157150269,\n",
              " 11420: 0.33678433299064636,\n",
              " 14974: 0.028559578582644463,\n",
              " 15267: 0.17866283655166626,\n",
              " 17801: 0.005640306044369936,\n",
              " 20716: 0.08004065603017807,\n",
              " 21198: 0.07718611508607864,\n",
              " 21995: 2.3444976806640625,\n",
              " 23723: 0.19007553160190582}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the ID position to text token mappings\n",
        "idx2token = {\n",
        "    idx: token for token, idx in tokenizer.get_vocab().items()\n",
        "}\n",
        "# map token IDs to human-readable tokens\n",
        "sparse_dict_tokens = {\n",
        "    idx2token[idx]: round(weight, 2) for idx, weight in zip(cols, weights)\n",
        "}\n",
        "# sort so we can see most relevant tokens first\n",
        "sparse_dict_tokens = {\n",
        "    k: v for k, v in sorted(\n",
        "        sparse_dict_tokens.items(),\n",
        "        key=lambda item: item[1],\n",
        "        reverse=True\n",
        "    )\n",
        "}\n",
        "sparse_dict_tokens\n",
        "\n",
        "#Caravanserais were roadside inns that were built along the Silk Road in areas including China, North Africa and the Middle East."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtAiUYsa-yRg",
        "outputId": "9a60c1ee-7a92-4a3e-9dc8-e2ccebff80cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dog': 2.58,\n",
              " 'dogs': 2.58,\n",
              " 'fishes': 2.34,\n",
              " 'fish': 2.25,\n",
              " 'like': 1.83,\n",
              " 'fishing': 1.75,\n",
              " 'liked': 1.05,\n",
              " 'eat': 0.72,\n",
              " 'behavior': 0.7,\n",
              " 'swimming': 0.68,\n",
              " 'want': 0.61,\n",
              " 'prefer': 0.52,\n",
              " 'they': 0.5,\n",
              " 'animal': 0.5,\n",
              " 'desert': 0.49,\n",
              " 'shark': 0.34,\n",
              " ':': 0.33,\n",
              " 'breed': 0.32,\n",
              " 'hunting': 0.31,\n",
              " 'animals': 0.3,\n",
              " 'ocean': 0.29,\n",
              " 'pet': 0.29,\n",
              " 'interesting': 0.28,\n",
              " 'bite': 0.25,\n",
              " 'smell': 0.23,\n",
              " 'zoo': 0.19,\n",
              " 'attract': 0.19,\n",
              " 'catfish': 0.19,\n",
              " 'bird': 0.18,\n",
              " 'predator': 0.18,\n",
              " 'tiger': 0.15,\n",
              " 'feed': 0.14,\n",
              " 'the': 0.13,\n",
              " 'species': 0.12,\n",
              " 'look': 0.1,\n",
              " 'swim': 0.09,\n",
              " 'bully': 0.08,\n",
              " 'parasite': 0.08,\n",
              " 'snake': 0.07,\n",
              " 'water': 0.06,\n",
              " 'yes': 0.06,\n",
              " 'roger': 0.03,\n",
              " 'aggression': 0.03,\n",
              " 'meat': 0.01,\n",
              " 'dolphin': 0.01,\n",
              " 'don': 0.0,\n",
              " '##fish': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"Caravanserais were also an important marketplace for commodities\",\n",
        "    \"From the 10th century onwards,as merchant and travel routes become more developed\",\n",
        "    \"Photosynthesis is the process of storing light energy as chemical energy in cells\",\n",
        "]"
      ],
      "metadata": {
        "id": "Iz_136T8Hjq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(\n",
        "    texts, return_tensors='pt',\n",
        "    padding=True, truncation=True\n",
        ")\n",
        "output = model(**tokens)\n",
        "# aggregate the token-level vecs and transform to sparse\n",
        "vecs = torch.max(\n",
        "    torch.log(1 + torch.relu(output.logits)) * tokens.attention_mask.unsqueeze(-1), dim=1\n",
        ").values.squeeze().detach().cpu().numpy()\n",
        "vecs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "ceibdff2IHwe",
        "outputId": "e4f9dd78-a729-4cf1-d159-bff00b80a902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5f0e6bf7b90>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tokens = tokenizer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "sim = np.zeros((vecs.shape[0], vecs.shape[0]))\n",
        "\n",
        "for i, vec in enumerate(vecs):\n",
        "    sim[i,:] = np.dot(vec, vecs.T) / (\n",
        "        np.linalg.norm(vec) * np.linalg.norm(vecs, axis=1)\n",
        "    )\n",
        "sim.round(decimals=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "qA_KdBARISXj",
        "outputId": "fda7e1e9-4a69-479f-a218-265be5fb22d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vecs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1dec368d92a7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vecs' is not defined"
          ]
        }
      ]
    }
  ]
}